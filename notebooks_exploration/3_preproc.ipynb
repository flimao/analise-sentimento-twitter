{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Pr√©-processamento e transforma√ß√µes\n",
    "\n",
    "Nessa fase, construiremos alguns modelos espec√≠ficos para texto para ent√£o trein√°-los;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# caminho para instala√ß√£o do pacote mltoolkit, com metricas e gr√°ficos personalizados\n",
    "# !pip install git+ssh://git@github.com/flimao/mltoolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import rcParams, rcParamsDefault, pyplot as plt\n",
    "import seaborn as sns\n",
    "from mltoolkit import metrics, plots, NLP\n",
    "import spacy\n",
    "\n",
    "rcParams.update(rcParamsDefault)\n",
    "rcParams['figure.dpi'] = 120\n",
    "rcParams['figure.figsize'] = (10, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download pt_core_news_lg\n",
    "# !python -m spacy download pt_core_news_md\n",
    "# !python -m spacy download pt_core_news_sm\n",
    "nlp = spacy.load(\"pt_core_news_lg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importa√ß√£o dos dados\n",
    "\n",
    "Primeiramente, importamos os dados e aplicamos as transforma√ß√µes utilizadas na fase anterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n√£o tocaremos no conjunto de submiss√£o\n",
    "\n",
    "tweets_raw = pd.read_csv(\n",
    "    r'../data/Train3Classes.csv',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trocar tipos para acelerar o processamento (menos espa√ßo em mem√≥ria)\n",
    "# e ativar poss√≠veis otimiza√ß√µes internas ao pandas para certos tipos\n",
    "def mudar_tipos(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    df['id'] = df['id'].astype('string')\n",
    "    df['tweet_date'] = pd.to_datetime(df['tweet_date'])\n",
    "    df['sentiment'] = df['sentiment'].astype('category')\n",
    "\n",
    "    return df\n",
    "\n",
    "def remover_duplicatas(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    df = df.drop_duplicates(subset = 'id')\n",
    "\n",
    "    return df\n",
    "\n",
    "# o √≠ndice √© o id, visto que n√£o h√° repetidos\n",
    "# vantagem: o √≠ndice √© removido automaticamente quando separamos em base de treino e teste.\n",
    "def setar_index(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    df = df.set_index('id')\n",
    "\n",
    "    return df\n",
    "\n",
    "tweets_full = (tweets_raw\n",
    "    .pipe(mudar_tipos)\n",
    "    .pipe(remover_duplicatas)\n",
    "    .pipe(setar_index)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pr√©-processamento de texto\n",
    "\n",
    "Vamos ent√£o implementar o pr√©-processamento do texto da fase anterior (An√°lise Explorat√≥ria de Texto).\n",
    "\n",
    "Primeiramente vamos importar as *stopwords*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'../data/stopwords_alopes.txt', encoding = 'utf8') as stopword_list:\n",
    "    lst = stopword_list.read().splitlines()\n",
    "\n",
    "stopwords_alopes = set([ stopword.strip() for stopword in lst ])\n",
    "\n",
    "# em uma an√°lise de sentimento, n√£o queremos remover palavras com conota√ß√£o negativa\n",
    "remover_stopwords = {\n",
    "    'n√£o', \n",
    "}\n",
    "\n",
    "stopwords_alopes -= remover_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_full = lambda s: NLP.preprocessing(s, preproc_funs_args = [\n",
    "    NLP.remove_links,\n",
    "    NLP.remove_hashtags,\n",
    "    NLP.remove_mentions,\n",
    "    NLP.remove_numbers,\n",
    "    NLP.remove_special_caract,\n",
    "    NLP.lowercase,\n",
    "    #remove_punkt,\n",
    "    #(remove_stopwords, dict(stopword_list = stopword_list_alopes)),\n",
    "    (NLP.tokenize_remove_stopwords_get_radicals_spacy, dict(\n",
    "        nlp = nlp,\n",
    "        stopword_list = stopwords_alopes,\n",
    "    )),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos ent√£o aplicar esse pr√©-processamento a uma amostra da base de *tweets* (para podermos iterar rapidamente caso necess√°rio). \n",
    "\n",
    "Em um momento posterior, treinaremos a base completa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "amostra_eda = 5000\n",
    "radicais = tweets_full.sample(amostra_eda)['tweet_text'].apply(preprocessing_full)\n",
    "\n",
    "tweets = tweets_full.copy()\n",
    "tweets['radicais'] = radicais\n",
    "tweets = tweets[tweets.radicais.notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>tweet_date</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>query_used</th>\n",
       "      <th>radicais</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1046253121217974272</th>\n",
       "      <td>queria beijar os dedinhso dos pes e das maos d...</td>\n",
       "      <td>2018-09-30 04:19:17+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>:(</td>\n",
       "      <td>querer beijar dedinhso pes maos deposi caro ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1046933383132192768</th>\n",
       "      <td>ai gente vai dar merda essa elei√ß√£o :(</td>\n",
       "      <td>2018-10-02 01:22:24+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>:(</td>\n",
       "      <td>ai gente merda eleicao</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1047555735192842240</th>\n",
       "      <td>@LuscaTurtle \"Pros senadores tenho que ainda q...</td>\n",
       "      <td>2018-10-03 18:35:24+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>:)</td>\n",
       "      <td>pros senador escolher outro alar suplicy lt pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1047468298554826752</th>\n",
       "      <td>d√≥lar derretendo !! R$3,848 com -2,09% !!! :))</td>\n",
       "      <td>2018-10-03 12:47:58+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>:)</td>\n",
       "      <td>dolar derreter r$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1042941480732581888</th>\n",
       "      <td>Servidoras do Itamaraty fizeram vaquinha para ...</td>\n",
       "      <td>2018-09-21 01:00:00+00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>jornaloglobo</td>\n",
       "      <td>servidor itamaraty fazer boi pagar cirurgia mu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1046925538282151936</th>\n",
       "      <td>Welder n√£o vai vim pra c√° come√ßo do ano :((((</td>\n",
       "      <td>2018-10-02 00:51:13+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>:(</td>\n",
       "      <td>welder nao vir pra ca comeco ano</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1049315231376269313</th>\n",
       "      <td>Eu quase chorei hj na sala de tanto que as gur...</td>\n",
       "      <td>2018-10-08 15:07:01+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>:(</td>\n",
       "      <td>quase chorar hj sala guri encher sacar excluiram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1046782264301015045</th>\n",
       "      <td>@yagodeluque üåª que lindo o perfil üíõ 8/10 ‚≠ê voc...</td>\n",
       "      <td>2018-10-01 15:21:54+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>:)</td>\n",
       "      <td>lindar perfil voce ta interagir comigo dia cha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048465998213537792</th>\n",
       "      <td>@GustavoHSalesVi @BlogdoNoblat Entendo q no wi...</td>\n",
       "      <td>2018-10-06 06:52:28+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>:)</td>\n",
       "      <td>entender q wikipedia so dia dar confusao</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1049223716482240513</th>\n",
       "      <td>@SputNico_ perdi :( Eu tava morrendo de dor de...</td>\n",
       "      <td>2018-10-08 09:03:22+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>:(</td>\n",
       "      <td>perder tava morrer dor cabeca dormir x_x sorry</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                            tweet_text  \\\n",
       "id                                                                       \n",
       "1046253121217974272  queria beijar os dedinhso dos pes e das maos d...   \n",
       "1046933383132192768             ai gente vai dar merda essa elei√ß√£o :(   \n",
       "1047555735192842240  @LuscaTurtle \"Pros senadores tenho que ainda q...   \n",
       "1047468298554826752     d√≥lar derretendo !! R$3,848 com -2,09% !!! :))   \n",
       "1042941480732581888  Servidoras do Itamaraty fizeram vaquinha para ...   \n",
       "1046925538282151936      Welder n√£o vai vim pra c√° come√ßo do ano :((((   \n",
       "1049315231376269313  Eu quase chorei hj na sala de tanto que as gur...   \n",
       "1046782264301015045  @yagodeluque üåª que lindo o perfil üíõ 8/10 ‚≠ê voc...   \n",
       "1048465998213537792  @GustavoHSalesVi @BlogdoNoblat Entendo q no wi...   \n",
       "1049223716482240513  @SputNico_ perdi :( Eu tava morrendo de dor de...   \n",
       "\n",
       "                                   tweet_date sentiment    query_used  \\\n",
       "id                                                                      \n",
       "1046253121217974272 2018-09-30 04:19:17+00:00         0            :(   \n",
       "1046933383132192768 2018-10-02 01:22:24+00:00         0            :(   \n",
       "1047555735192842240 2018-10-03 18:35:24+00:00         1            :)   \n",
       "1047468298554826752 2018-10-03 12:47:58+00:00         1            :)   \n",
       "1042941480732581888 2018-09-21 01:00:00+00:00         2  jornaloglobo   \n",
       "1046925538282151936 2018-10-02 00:51:13+00:00         0            :(   \n",
       "1049315231376269313 2018-10-08 15:07:01+00:00         0            :(   \n",
       "1046782264301015045 2018-10-01 15:21:54+00:00         1            :)   \n",
       "1048465998213537792 2018-10-06 06:52:28+00:00         1            :)   \n",
       "1049223716482240513 2018-10-08 09:03:22+00:00         0            :(   \n",
       "\n",
       "                                                              radicais  \n",
       "id                                                                      \n",
       "1046253121217974272  querer beijar dedinhso pes maos deposi caro ba...  \n",
       "1046933383132192768                             ai gente merda eleicao  \n",
       "1047555735192842240  pros senador escolher outro alar suplicy lt pr...  \n",
       "1047468298554826752                                  dolar derreter r$  \n",
       "1042941480732581888  servidor itamaraty fazer boi pagar cirurgia mu...  \n",
       "1046925538282151936                   welder nao vir pra ca comeco ano  \n",
       "1049315231376269313   quase chorar hj sala guri encher sacar excluiram  \n",
       "1046782264301015045  lindar perfil voce ta interagir comigo dia cha...  \n",
       "1048465998213537792           entender q wikipedia so dia dar confusao  \n",
       "1049223716482240513     perder tava morrer dor cabeca dormir x_x sorry  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Op√ß√µes de modelos\n",
    "\n",
    "Vamos agora olhar para alguns modelos que podemos utilizar.\n",
    "\n",
    "Definiremos os modelos desejados, e ent√£o procederemos √† compara√ß√£o dos mesmos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tweets['radicais']\n",
    "y = tweets['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size = 0.3,\n",
    "    stratify = y,\n",
    ")\n",
    "\n",
    "X_trains = {}\n",
    "X_tests = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. *Bag of Words* / `CountVectorizer`\n",
    "\n",
    "*Bag of Words* √© o processo onde traduzimos o texto j√° tratado para uma representa√ß√£o num√©rica que fa√ßa sentido para o modelo de *Machine Learning* consiga interpret√°-lo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer()\n",
    "X_trains['cv'] = cv.fit_transform(X_train).toarray()\n",
    "X_tests['cv'] = cv.transform(X_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_trains['cv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3500, 6419)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_trains['cv'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. TF-IDF\n",
    "\n",
    "***Term Frequency and Inverse Document Frequency*** √© uma tranforma√ß√£o onde avaliamos a relev√¢ncia das palavras pela **Frequ√™ncia dos Termos** e multiplicamos pelo **Inverso da Frequ√™ncia nos Documentos**.\n",
    "\n",
    "Nesse contexto, um **documento** √© cada um dos textos dentro de um *dataset*. Vamos entender cada um dos termos:\n",
    "\n",
    "> **TF - Term Frequency**: √© a frequ√™ncia de vezes que um termo/palavra aparece em cada um dos documentos analisados (isso nos ajuda a avaliar a relev√¢ncia daquela palavra);\n",
    "\n",
    "> **IDF - Inverse Document Frequency**: aqui avaliamos em quantos documentos o termo/palavra aparece (dessa forma conseguimos entender a sua influ√™ncia em identificar os textos);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(use_idf = True)\n",
    "\n",
    "X_trains['tfidf'] = tfidf.fit_transform(X_train).todense()\n",
    "X_tests['tfidf']  = tfidf.transform(X_test).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_trains['tfidf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3500, 6419)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_trains['tfidf'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Word2vec*\n",
    "\n",
    "O *Word2vec* ([Wikipedia](https://en.wikipedia.org/wiki/Word2vec), [Gensim](https://radimrehurek.com/gensim/models/word2vec.html)) √© uma rede neural onde associa-se vetores a cada palavra. Os vetores s√£o tais que pretendem capturar as rela√ß√µes sem√¢nticas entre as mesmas.\n",
    "\n",
    "Por exemplo, se tivermos em nosso vocabul√°rio as palavras *rei*, *rainha*, *homem* e *mulher*, poder√≠amos fazer a seguinte opera√ß√£o vetorial:\n",
    "\n",
    "$ \\vec{v}_{rei} - \\vec{v}_{homem} + \\vec{v}_{mulher} = \\vec{v}_{rainha}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "X_train_tokens = X_train.str.split(' ').to_list()\n",
    "X_test_tokens = X_test.str.split(' ').to_list()\n",
    "\n",
    "w2v_model = Word2Vec(\n",
    "    sentences = X_train_tokens, \n",
    "    vector_size = 2,  # este par√¢metro √© o equivalente ao n√∫mero de features. \n",
    "    min_count = 1, \n",
    "    workers = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bgl', 0.9999997615814209),\n",
       " ('itabirito', 0.9999997019767761),\n",
       " ('fundir', 0.9999992251396179),\n",
       " ('eai', 0.9999991655349731),\n",
       " ('adolescencia', 0.9999955892562866),\n",
       " ('humilhacao', 0.9999939799308777),\n",
       " ('aquii', 0.9999939799308777),\n",
       " ('store', 0.9999932646751404),\n",
       " ('assistir', 0.9999839067459106),\n",
       " ('beija-mao', 0.9999828338623047)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive = ['bolsonaro'], negative = ['haddad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97749794"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.similarity('bolsonaro', 'haddad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fun√ß√£o para, dado um modelo word2vec e um conjunto de frases em formato de token (listas de listas), \n",
    "# construir os vetores associados a cada uma\n",
    "\n",
    "def build_word2vec_vectors(model, phrases, vector_combination):\n",
    "\n",
    "    X = []\n",
    "    vector_size = model.vector_size\n",
    "\n",
    "    for phrase in phrases:\n",
    "\n",
    "        ntokens = len(phrase)\n",
    "        vectors = np.zeros(shape = (ntokens, vector_size))\n",
    "\n",
    "        for i, token in enumerate(phrase):\n",
    "            try:\n",
    "\n",
    "                vectors[i, :] = model.wv[token]\n",
    "            except KeyError:  # token not present in corpus\n",
    "                vectors[i, :] = 0\n",
    "\n",
    "        X.append(vector_combination(vectors))\n",
    "    \n",
    "    return np.asarray(X)\n",
    "\n",
    "# fun√ß√£o para, dados conjuntos de frases de treino e teste, construir os vetores\n",
    "# associados\n",
    "def build_word2vec_model(\n",
    "    X_train, X_test, \n",
    "    vector_combination,\n",
    "    is_token = False,\n",
    "    **kwargs\n",
    "):\n",
    "    # kwargs = arguments for Word2Vec class\n",
    "    \n",
    "    if is_token:\n",
    "        X_train_tokens = X_train\n",
    "        X_test_tokens = X_test\n",
    "    else:\n",
    "        X_train_tokens = X_train.str.split(' ').to_list()\n",
    "        X_test_tokens = X_test.str.split(' ').to_list()\n",
    "    \n",
    "    # instantiate, build and train model\n",
    "    w2v_model = Word2Vec(\n",
    "        sentences = X_train_tokens, \n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    # build vectors\n",
    "    X_train_w2v = build_word2vec_vectors(\n",
    "        model = w2v_model, \n",
    "        phrases = X_train_tokens, \n",
    "        vector_combination = vector_combination\n",
    "    )\n",
    "\n",
    "    X_test_w2v = build_word2vec_vectors(\n",
    "        model = w2v_model, \n",
    "        phrases = X_test_tokens, \n",
    "        vector_combination = vector_combination\n",
    "    )\n",
    "\n",
    "    return w2v_model, X_train_w2v, X_test_w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "1043224595644522496    sinal importante detectar area buscar submarin...\n",
       "Name: radicais, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model, X_trains['word2vec_sum'], X_tests['word2vec'] = build_word2vec_model(\n",
    "    X_train[:2], X_test[:2], \n",
    "    is_token = False,\n",
    "    # --- word2vec model parameters\n",
    "    # vector_size = 2, # este par√¢metro √© o equivalente ao n√∫mero de features. \n",
    "    # min_count = 1, workers = 2,\n",
    "    vector_size=2, alpha=0.025, window=5, min_count=1, max_vocab_size=None, sample=1e-3, seed=1,\n",
    "                 workers=2, min_alpha=0.0001, sg=0, hs=0, negative=5, cbow_mean=1, hashfxn=hash, epochs=5, null_word=0,\n",
    "                 trim_rule=None, sorted_vocab=1, batch_words=10000,\n",
    "    # --- vector_combination\n",
    "    vector_combination = lambda x: np.sum(x, axis = 0),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.09479237,  0.56492567],\n",
       "       [-0.02681136,  0.01182151]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_trains['word2vec_sum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from sklearn.exceptions import NotFittedError\n",
    "\n",
    "from gensim import models\n",
    "\n",
    "class W2VTransformer(TransformerMixin, BaseEstimator):\n",
    "    \"\"\"Base Word2Vec module, wraps :class:`~gensim.models.word2vec.Word2Vec`.\n",
    "    For more information please have a look to `Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean: \"Efficient\n",
    "    Estimation of Word Representations in Vector Space\" <https://arxiv.org/abs/1301.3781>`_.\n",
    "    \"\"\"\n",
    "    def __init__(self, vector_size=100, alpha=0.025, window=5, min_count=5, max_vocab_size=None, sample=1e-3, seed=1,\n",
    "                 workers=3, min_alpha=0.0001, sg=0, hs=0, negative=5, cbow_mean=1, hashfxn=hash, epochs=5, null_word=0,\n",
    "                 trim_rule=None, sorted_vocab=1, batch_words=10000, vector_combination = lambda x: np.sum(x, axis = 0)):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        size : int\n",
    "            Dimensionality of the feature vectors.\n",
    "        alpha : float\n",
    "            The initial learning rate.\n",
    "        window : int\n",
    "            The maximum distance between the current and predicted word within a sentence.\n",
    "        min_count : int\n",
    "            Ignores all words with total frequency lower than this.\n",
    "        max_vocab_size : int\n",
    "            Limits the RAM during vocabulary building; if there are more unique\n",
    "            words than this, then prune the infrequent ones. Every 10 million word types need about 1GB of RAM.\n",
    "            Set to `None` for no limit.\n",
    "        sample : float\n",
    "            The threshold for configuring which higher-frequency words are randomly downsampled,\n",
    "            useful range is (0, 1e-5).\n",
    "        seed : int\n",
    "            Seed for the random number generator. Initial vectors for each word are seeded with a hash of\n",
    "            the concatenation of word + `str(seed)`. Note that for a fully deterministically-reproducible run,\n",
    "            you must also limit the model to a single worker thread (`workers=1`), to eliminate ordering jitter\n",
    "            from OS thread scheduling. (In Python 3, reproducibility between interpreter launches also requires\n",
    "            use of the `PYTHONHASHSEED` environment variable to control hash randomization).\n",
    "        workers : int\n",
    "            Use these many worker threads to train the model (=faster training with multicore machines).\n",
    "        min_alpha : float\n",
    "            Learning rate will linearly drop to `min_alpha` as training progresses.\n",
    "        sg : int {1, 0}\n",
    "            Defines the training algorithm. If 1, CBOW is used, otherwise, skip-gram is employed.\n",
    "        hs : int {1,0}\n",
    "            If 1, hierarchical softmax will be used for model training.\n",
    "            If set to 0, and `negative` is non-zero, negative sampling will be used.\n",
    "        negative : int\n",
    "            If > 0, negative sampling will be used, the int for negative specifies how many \"noise words\"\n",
    "            should be drawn (usually between 5-20).\n",
    "            If set to 0, no negative sampling is used.\n",
    "        cbow_mean : int {1,0}\n",
    "            If 0, use the sum of the context word vectors. If 1, use the mean, only applies when cbow is used.\n",
    "        hashfxn : callable (object -> int), optional\n",
    "            A hashing function. Used to create an initial random reproducible vector by hashing the random seed.\n",
    "        iter : int\n",
    "            Number of iterations (epochs) over the corpus.\n",
    "        null_word : int {1, 0}\n",
    "            If 1, a null pseudo-word will be created for padding when using concatenative L1 (run-of-words)\n",
    "        trim_rule : function\n",
    "            Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
    "            be trimmed away, or handled using the default (discard if word count < min_count).\n",
    "            Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
    "            or a callable that accepts parameters (word, count, min_count) and returns either\n",
    "            :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
    "            Note: The rule, if given, is only used to prune vocabulary during build_vocab() and is not stored as part\n",
    "            of the model.\n",
    "        sorted_vocab : int {1,0}\n",
    "            If 1, sort the vocabulary by descending frequency before assigning word indexes.\n",
    "        batch_words : int\n",
    "            Target size (in words) for batches of examples passed to worker threads (and\n",
    "            thus cython routines).(Larger batches will be passed if individual\n",
    "            texts are longer than 10000 words, but the standard cython code truncates to that maximum.)\n",
    "        \"\"\"\n",
    "        self.gensim_model = None\n",
    "        self.vector_size = vector_size\n",
    "        self.alpha = alpha\n",
    "        self.window = window\n",
    "        self.min_count = min_count\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.sample = sample\n",
    "        self.seed = seed\n",
    "        self.workers = workers\n",
    "        self.min_alpha = min_alpha\n",
    "        self.sg = sg\n",
    "        self.hs = hs\n",
    "        self.negative = negative\n",
    "        self.cbow_mean = int(cbow_mean)\n",
    "        self.hashfxn = hashfxn\n",
    "        self.epochs = epochs\n",
    "        self.null_word = null_word\n",
    "        self.trim_rule = trim_rule\n",
    "        self.sorted_vocab = sorted_vocab\n",
    "        self.batch_words = batch_words\n",
    "        self.vector_combination = vector_combination\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Fit the model according to the given training data.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : iterable of iterables of str\n",
    "            The input corpus. X can be simply a list of lists of tokens, but for larger corpora,\n",
    "            consider an iterable that streams the sentences directly from disk/network.\n",
    "            See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
    "            or :class:`~gensim.models.word2vec.LineSentence` in :mod:`~gensim.models.word2vec` module for such examples.\n",
    "        Returns\n",
    "        -------\n",
    "        :class:`~gensim.sklearn_api.w2vmodel.W2VTransformer`\n",
    "            The trained model.\n",
    "        \"\"\"\n",
    "\n",
    "        X_tokens = X.str.split(' ').to_list()\n",
    "\n",
    "        self.gensim_model = models.Word2Vec(\n",
    "            sentences=X_tokens, vector_size=self.vector_size, alpha=self.alpha,\n",
    "            window=self.window, min_count=self.min_count, max_vocab_size=self.max_vocab_size,\n",
    "            sample=self.sample, seed=self.seed, workers=self.workers, min_alpha=self.min_alpha,\n",
    "            sg=self.sg, hs=self.hs, negative=self.negative, cbow_mean=self.cbow_mean,\n",
    "            hashfxn=self.hashfxn, epochs=self.epochs, null_word=self.null_word, trim_rule=self.trim_rule,\n",
    "            sorted_vocab=self.sorted_vocab, batch_words=self.batch_words\n",
    "        )\n",
    "        return self\n",
    "\n",
    "    def transform(self, words):\n",
    "        \"\"\"Get the word vectors the input words.\n",
    "        Parameters\n",
    "        ----------\n",
    "        words : {iterable of str, str}\n",
    "            Word or a collection of words to be transformed.\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray of shape [`len(words)`, `size`]\n",
    "            A 2D array where each row is the vector of one word.\n",
    "        \"\"\"\n",
    "        if self.gensim_model is None:\n",
    "            raise NotFittedError(\n",
    "                \"This model has not been fitted yet. Call 'fit' with appropriate arguments before using this method.\"\n",
    "            )\n",
    "\n",
    "        # # The input as array of array\n",
    "        # if isinstance(words, six.string_types):\n",
    "        #     words = [words]\n",
    "        # vectors = [self.gensim_model.wv[word] for word in words]\n",
    "        # return np.reshape(np.array(vectors), (len(words), self.vector_size))\n",
    "\n",
    "        phrases = words.str.split(' ').to_list()\n",
    "\n",
    "        wvs = build_word2vec_vectors(\n",
    "            model = self.gensim_model,\n",
    "            phrases = phrases,\n",
    "            vector_combination = self.vector_combination\n",
    "        )\n",
    "\n",
    "        return wvs\n",
    "\n",
    "    def partial_fit(self, X):\n",
    "        raise NotImplementedError(\n",
    "            \"'partial_fit' has not been implemented for W2VTransformer. \"\n",
    "            \"However, the model can be updated with a fixed vocabulary using Gensim API call.\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mltoolkit.NLP import W2VTransformer as WtoVTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mltoolkit_NLP' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8128/2196405586.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmltoolkit_NLP\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW2VTransformer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'mltoolkit_NLP' is not defined"
     ]
    }
   ],
   "source": [
    "mltoolkit_NLP.W2VTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['perder', 'correntinha', 'ourar', 'o', 'trilho'],\n",
       " ['lancamento',\n",
       "  'single',\n",
       "  'conflito',\n",
       "  'noticiar',\n",
       "  'portal',\n",
       "  'voce',\n",
       "  'nao',\n",
       "  'ouvir',\n",
       "  'acesse',\n",
       "  'aumentar',\n",
       "  'som']]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:2].str.split(' ').to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vt = W2VTransformer(    \n",
    "    # --- word2vec model parameters\n",
    "    vector_size = 2, # este par√¢metro √© o equivalente ao n√∫mero de features. \n",
    "    min_count = 1, workers = 2,\n",
    ")\n",
    "\n",
    "xtr = w2vt.fit_transform(X_train[:2])\n",
    "xtst = w2vt.transform(X_test[:2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.09479237,  0.56492567],\n",
       "       [-0.02681136,  0.01182151]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vtm = WtoVTransformer(    \n",
    "    # --- word2vec model parameters\n",
    "    vector_size = 2, # este par√¢metro √© o equivalente ao n√∫mero de features. \n",
    "    min_count = 1, workers = 2,\n",
    ")\n",
    "\n",
    "xtr = w2vtm.fit_transform(X_train[:2])\n",
    "xtst = w2vtm.transform(X_test[:2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.09479237,  0.56492567],\n",
       "       [-0.02681136,  0.01182151]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Doc2Vec*\n",
    "\n",
    "O *Doc2Vec* ([Gensim](https://radimrehurek.com/gensim/auto_examples/tutorials/run_doc2vec_lee.html)) √© um modelo similar ao *Word2Vec*, mas que leva em considera√ß√£o tamb√©m o contexto de cada frase na constru√ß√£o dos vetores de similaridade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import doc2vec\n",
    "\n",
    "# fun√ß√£o para ler o corpus e tagear os documentos (no caso, tweets)\n",
    "def read_corpus(list_sentences, tokens_only = False):\n",
    "    if tokens_only:\n",
    "        return list_sentences\n",
    "    else:\n",
    "        # For training data, add tags\n",
    "        lista = []\n",
    "        for i, line in enumerate(list_sentences):\n",
    "            lista.append(doc2vec.TaggedDocument(line, [i]))\n",
    "\n",
    "        return lista\n",
    "    \n",
    "train_corpus = read_corpus(X_train_tokens)\n",
    "test_corpus = read_corpus(X_test_tokens, tokens_only = True)\n",
    "\n",
    "d2v_model = doc2vec.Doc2Vec(vector_size = 50, min_count = 2, epochs = 20)\n",
    "\n",
    "d2v_model.build_vocab(train_corpus)\n",
    "\n",
    "d2v_model.train(\n",
    "    train_corpus, \n",
    "    total_examples = d2v_model.corpus_count, \n",
    "    epochs = d2v_model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.7442044e-02,  2.5535559e-02,  2.3881635e-02, -1.2053747e-02,\n",
       "       -2.3669552e-02, -3.6800306e-02,  2.2385698e-02,  4.4900913e-02,\n",
       "       -5.1939674e-02, -5.1815566e-03, -1.0489226e-02, -3.7510429e-02,\n",
       "       -6.9495644e-03, -3.1758684e-03, -1.2563300e-02, -1.6216686e-02,\n",
       "        4.1447401e-02, -3.3214556e-03, -5.2924678e-02, -1.9637613e-02,\n",
       "       -4.1356515e-03,  1.9877413e-02,  5.5215832e-02, -7.6075710e-05,\n",
       "        2.5362268e-02,  1.2883319e-02, -2.8340450e-02,  9.9135889e-04,\n",
       "       -3.4626570e-02,  7.1958336e-03,  2.1095302e-02, -2.6432965e-02,\n",
       "       -2.4708290e-02,  2.3136113e-02, -1.8435480e-02,  3.7445284e-02,\n",
       "        2.1635253e-02,  1.3988094e-02,  1.2153922e-02, -1.2700081e-02,\n",
       "        3.7212495e-02, -1.9925709e-03, -1.2544175e-02, -1.2507794e-03,\n",
       "        6.6553988e-02,  1.7250776e-02, -1.5909132e-02, -5.1287767e-02,\n",
       "        1.4884579e-02,  1.6710229e-02], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exemplo: vetor de uma frase contendo duas palavras: 'bolsonaro' e 'haddad'\n",
    "\n",
    "d2v_model.infer_vector(['bolsonaro', 'haddad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fun√ß√£o para, dado um modelo doc2vec e um conjunto de tokens, construir o vetor associado\n",
    "def build_doc2vec_vector(d2v_model, phrases):\n",
    "    X = []\n",
    "\n",
    "    for phrase in phrases:\n",
    "        vecs = []\n",
    "        vecs.append(d2v_model.infer_vector(phrase))\n",
    "        \n",
    "        X.append(vecs)\n",
    "        \n",
    "    X_d2v = np.array(X)[:, 0, :]\n",
    "\n",
    "    return X_d2v\n",
    "\n",
    "# fun√ß√£o para, dados conjuntos de frases de treino e teste, construir os vetores\n",
    "# associados\n",
    "\n",
    "def build_doc2vec_model(\n",
    "    X_train, X_test, \n",
    "    is_token = False,\n",
    "    **kwargs\n",
    "):\n",
    "\n",
    "    if is_token:\n",
    "        X_train_tokens = X_train\n",
    "        X_test_tokens = X_test\n",
    "    else:\n",
    "        X_train_tokens = X_train.str.split(' ').to_list()\n",
    "        X_test_tokens = X_test.str.split(' ').to_list()\n",
    "    \n",
    "    # make corpus\n",
    "    train_corpus = read_corpus(X_train_tokens)\n",
    "    test_corpus = read_corpus(X_test_tokens, tokens_only = True)\n",
    "\n",
    "    # instantiate doc2vec model\n",
    "    d2v_model = doc2vec.Doc2Vec(**kwargs)\n",
    "\n",
    "    # build vocabulary\n",
    "    d2v_model.build_vocab(train_corpus)\n",
    "\n",
    "    # train model\n",
    "    d2v_model.train(\n",
    "        train_corpus, \n",
    "        total_examples = d2v_model.corpus_count, \n",
    "        epochs = d2v_model.epochs\n",
    "    )\n",
    "\n",
    "    # build vectors\n",
    "    X_train_d2v = build_doc2vec_vector(\n",
    "        d2v_model = d2v_model, \n",
    "        phrases = X_train_tokens\n",
    "    )\n",
    "    X_test_d2v = build_doc2vec_vector(\n",
    "        d2v_model = d2v_model, \n",
    "        phrases = X_test_tokens\n",
    "    )\n",
    "\n",
    "    return d2v_model, X_train_d2v, X_test_d2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2v_model, X_train_d2v, X_test_d2v = build_doc2vec_model(\n",
    "    X_train, X_test, \n",
    "    is_token = False,\n",
    "    vector_size = 50, min_count = 2, epochs = 20,  # doc2vec model arguments\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3500, 50)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_d2v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1500, 50)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_d2v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ecb817f0115fcb91f0cf82d46898b9ac90f725ea2a7dc8e9ce55807017ad6340"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('dsd': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
