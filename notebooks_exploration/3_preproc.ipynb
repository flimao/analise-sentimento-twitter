{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Pré-processamento e transformações\n",
    "\n",
    "Nessa fase, construiremos alguns modelos específicos para texto para então treiná-los;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# caminho para instalação do pacote mltoolkit, com metricas e gráficos personalizados\n",
    "# !pip install git+ssh://git@github.com/flimao/mltoolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import rcParams, rcParamsDefault, pyplot as plt\n",
    "import seaborn as sns\n",
    "from mltoolkit import metrics, plots, NLP\n",
    "import spacy\n",
    "\n",
    "rcParams.update(rcParamsDefault)\n",
    "rcParams['figure.dpi'] = 120\n",
    "rcParams['figure.figsize'] = (10, 8)\n",
    "\n",
    "nlp = spacy.load(\"pt_core_news_lg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importação dos dados\n",
    "\n",
    "Primeiramente, importamos os dados e aplicamos as transformações utilizadas na fase anterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# não tocaremos no conjunto de submissão\n",
    "\n",
    "tweets_raw = pd.read_csv(\n",
    "    r'../data/Train3Classes.csv',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trocar tipos para acelerar o processamento (menos espaço em memória)\n",
    "# e ativar possíveis otimizações internas ao pandas para certos tipos\n",
    "def mudar_tipos(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    df['id'] = df['id'].astype('string')\n",
    "    df['tweet_date'] = pd.to_datetime(df['tweet_date'])\n",
    "    df['sentiment'] = df['sentiment'].astype('category')\n",
    "\n",
    "    return df\n",
    "\n",
    "def remover_duplicatas(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    df = df.drop_duplicates(subset = 'id')\n",
    "\n",
    "    return df\n",
    "\n",
    "# o índice é o id, visto que não há repetidos\n",
    "# vantagem: o índice é removido automaticamente quando separamos em base de treino e teste.\n",
    "def setar_index(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    df = df.set_index('id')\n",
    "\n",
    "    return df\n",
    "\n",
    "tweets_full = (tweets_raw\n",
    "    .pipe(mudar_tipos)\n",
    "    .pipe(remover_duplicatas)\n",
    "    .pipe(setar_index)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pré-processamento de texto\n",
    "\n",
    "Vamos então implementar o pré-processamento do texto da fase anterior (Análise Exploratória de Texto).\n",
    "\n",
    "Primeiramente vamos importar as *stopwords*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'../data/stopwords_alopes.txt', encoding = 'utf8') as stopword_list:\n",
    "    lst = stopword_list.read().splitlines()\n",
    "\n",
    "stopwords_alopes = set([ stopword.strip() for stopword in lst ])\n",
    "\n",
    "# em uma análise de sentimento, não queremos remover palavras com conotação negativa\n",
    "remover_stopwords = {\n",
    "    'não', \n",
    "}\n",
    "\n",
    "stopwords_alopes -= remover_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_full = lambda s: NLP.preprocessing(s, preproc_funs_args = [\n",
    "    NLP.remove_links,\n",
    "    NLP.remove_hashtags,\n",
    "    NLP.remove_mentions,\n",
    "    NLP.remove_numbers,\n",
    "    NLP.remove_special_caract,\n",
    "    NLP.lowercase,\n",
    "    #remove_punkt,\n",
    "    #(remove_stopwords, dict(stopword_list = stopword_list_alopes)),\n",
    "    (NLP.tokenize_remove_stopwords_get_radicais_spacy, dict(\n",
    "        nlp = nlp,\n",
    "        stopword_list = stopwords_alopes,\n",
    "    )),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos então aplicar esse pré-processamento a uma amostra da base de *tweets* (para podermos iterar rapidamente caso necessário). \n",
    "\n",
    "Em um momento posterior, treinaremos a base completa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "amostra_eda = 5000\n",
    "radicais = tweets_full.sample(amostra_eda)['tweet_text'].apply(preprocessing_full)\n",
    "\n",
    "tweets = tweets_full.copy()\n",
    "tweets['radicais'] = radicais\n",
    "tweets = tweets[tweets.radicais.notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>tweet_date</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>query_used</th>\n",
       "      <th>radicais</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1034257540736065537</th>\n",
       "      <td>Olha eu preparando um novo projeto gráfico. Pa...</td>\n",
       "      <td>2018-08-28 01:53:07+00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>#trabalho</td>\n",
       "      <td>olhar preparar projeto grafico pausar pra foti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1050742090152468480</th>\n",
       "      <td>@lfrancobastos Porque não atendeste? Fifa ? :P</td>\n",
       "      <td>2018-10-12 13:36:50+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>:)</td>\n",
       "      <td>nao atender fifa :p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1046254666147860481</th>\n",
       "      <td>Fizemos o grupinho das ARMYs de cwb e ele já f...</td>\n",
       "      <td>2018-09-30 04:25:25+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>:(</td>\n",
       "      <td>fazer grupo armys cwb ja flopou querer convers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1030566551357927424</th>\n",
       "      <td>Assistente Operacional (Emissão de NF) - Serra...</td>\n",
       "      <td>2018-08-17 21:26:27+00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>#oportunidade</td>\n",
       "      <td>assistente operacional emissao nf serrar es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1049058097610854400</th>\n",
       "      <td>Ibope: Boca de urna dá segundo turno entre Bol...</td>\n",
       "      <td>2018-10-07 22:05:15+00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>jornaloglobo</td>\n",
       "      <td>ibope bocar urna turno bolsonaro haddad presid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1031482217548144642</th>\n",
       "      <td>BRASIL/EMPREGOS Construa seu futuro conosco: o...</td>\n",
       "      <td>2018-08-20 10:04:59+00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>#trabalho</td>\n",
       "      <td>brasil emprego construir futurar conosco ofert...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1045392580949741568</th>\n",
       "      <td>@RIC4RDO_2479 @anais_lachado18 Btw tu não sabe...</td>\n",
       "      <td>2018-09-27 19:19:48+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>:)</td>\n",
       "      <td>btw nao saber gostar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1050752935603384320</th>\n",
       "      <td>@juzaraujo Que daora! Fico contente que tenha ...</td>\n",
       "      <td>2018-10-12 14:19:56+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>:)</td>\n",
       "      <td>daora ficar contentar dar certar querer ha tri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1047492196419686401</th>\n",
       "      <td>@Erickaolol O básico é ganhar dos caras ir lá ...</td>\n",
       "      <td>2018-10-03 14:22:55+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>:)</td>\n",
       "      <td>basico ganhar caro o melhor ta mundo mal kabum...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1050722239090515971</th>\n",
       "      <td>vou reler minha conversa com o Sung pra tentar...</td>\n",
       "      <td>2018-10-12 12:17:57+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>:)</td>\n",
       "      <td>ir reler converso sung pra lembrar d</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                            tweet_text  \\\n",
       "id                                                                       \n",
       "1034257540736065537  Olha eu preparando um novo projeto gráfico. Pa...   \n",
       "1050742090152468480     @lfrancobastos Porque não atendeste? Fifa ? :P   \n",
       "1046254666147860481  Fizemos o grupinho das ARMYs de cwb e ele já f...   \n",
       "1030566551357927424  Assistente Operacional (Emissão de NF) - Serra...   \n",
       "1049058097610854400  Ibope: Boca de urna dá segundo turno entre Bol...   \n",
       "1031482217548144642  BRASIL/EMPREGOS Construa seu futuro conosco: o...   \n",
       "1045392580949741568  @RIC4RDO_2479 @anais_lachado18 Btw tu não sabe...   \n",
       "1050752935603384320  @juzaraujo Que daora! Fico contente que tenha ...   \n",
       "1047492196419686401  @Erickaolol O básico é ganhar dos caras ir lá ...   \n",
       "1050722239090515971  vou reler minha conversa com o Sung pra tentar...   \n",
       "\n",
       "                                   tweet_date sentiment     query_used  \\\n",
       "id                                                                       \n",
       "1034257540736065537 2018-08-28 01:53:07+00:00         2      #trabalho   \n",
       "1050742090152468480 2018-10-12 13:36:50+00:00         1             :)   \n",
       "1046254666147860481 2018-09-30 04:25:25+00:00         0             :(   \n",
       "1030566551357927424 2018-08-17 21:26:27+00:00         2  #oportunidade   \n",
       "1049058097610854400 2018-10-07 22:05:15+00:00         2   jornaloglobo   \n",
       "1031482217548144642 2018-08-20 10:04:59+00:00         2      #trabalho   \n",
       "1045392580949741568 2018-09-27 19:19:48+00:00         1             :)   \n",
       "1050752935603384320 2018-10-12 14:19:56+00:00         1             :)   \n",
       "1047492196419686401 2018-10-03 14:22:55+00:00         1             :)   \n",
       "1050722239090515971 2018-10-12 12:17:57+00:00         1             :)   \n",
       "\n",
       "                                                              radicais  \n",
       "id                                                                      \n",
       "1034257540736065537  olhar preparar projeto grafico pausar pra foti...  \n",
       "1050742090152468480                                nao atender fifa :p  \n",
       "1046254666147860481  fazer grupo armys cwb ja flopou querer convers...  \n",
       "1030566551357927424        assistente operacional emissao nf serrar es  \n",
       "1049058097610854400  ibope bocar urna turno bolsonaro haddad presid...  \n",
       "1031482217548144642  brasil emprego construir futurar conosco ofert...  \n",
       "1045392580949741568                               btw nao saber gostar  \n",
       "1050752935603384320  daora ficar contentar dar certar querer ha tri...  \n",
       "1047492196419686401  basico ganhar caro o melhor ta mundo mal kabum...  \n",
       "1050722239090515971               ir reler converso sung pra lembrar d  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opções de modelos\n",
    "\n",
    "Vamos agora olhar para alguns modelos que podemos utilizar.\n",
    "\n",
    "Definiremos os modelos desejados, e então procederemos à comparação dos mesmos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tweets['radicais']\n",
    "y = tweets['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size = 0.3,\n",
    "    stratify = y,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. *Bag of Words* / `CountVectorizer`\n",
    "\n",
    "*Bag of Words* é o processo onde traduzimos o texto já tratado para uma representação numérica que faça sentido para o modelo de *Machine Learning* consiga interpretá-lo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer()\n",
    "X_train_cv = cv.fit_transform(X_train).toarray()\n",
    "X_test_cv = cv.transform(X_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3500, 6191)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_cv.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. TF-IDF\n",
    "\n",
    "***Term Frequency and Inverse Document Frequency*** é uma tranformação onde avaliamos a relevância das palavras pela **Frequência dos Termos** e multiplicamos pelo **Inverso da Frequência nos Documentos**.\n",
    "\n",
    "Nesse contexto, um **documento** é cada um dos textos dentro de um *dataset*. Vamos entender cada um dos termos:\n",
    "\n",
    "> **TF - Term Frequency**: é a frequência de vezes que um termo/palavra aparece em cada um dos documentos analisados (isso nos ajuda a avaliar a relevância daquela palavra);\n",
    "\n",
    "> **IDF - Inverse Document Frequency**: aqui avaliamos em quantos documentos o termo/palavra aparece (dessa forma conseguimos entender a sua influência em identificar os textos);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(use_idf = True)\n",
    "\n",
    "X_train_tfidf = tfidf.fit_transform(X_train).todense()\n",
    "X_test_tfidf  = tfidf.transform(X_test).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3500, 6191)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Word2vec*\n",
    "\n",
    "O *Word2vec* é um modelo onde associa-se vetores a cada palavra. Os vetores são tais que pretendem capturar as relações semânticas entre as mesmas.\n",
    "\n",
    "Por exemplo, se tivermos em nosso vocabulário as palavras *rei*, *rainha*, *homem* e *mulher*, poderíamos fazer a seguinte operação vetorial:\n",
    "\n",
    "$ \\vec{v}_{rei} - \\vec{v}_{homem} + \\vec{v}_{mulher} = \\vec{v}_{rainha}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "X_train_tokens = X_train.str.split(' ').to_list()\n",
    "X_test_tokens = X_test.str.split(' ').to_list()\n",
    "\n",
    "w2v_model = Word2Vec(\n",
    "    sentences = X_train_tokens, \n",
    "    vector_size = 2,  # este parâmetro é o equivalente ao número de features. \n",
    "    min_count = 1, \n",
    "    workers = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('thrones', 1.0),\n",
       " ('rafaela', 1.0),\n",
       " ('anem', 0.9999998807907104),\n",
       " ('vendido', 0.9999968409538269),\n",
       " ('hyunjinni', 0.9999948740005493),\n",
       " ('matematicamente', 0.9999947547912598),\n",
       " ('leriar', 0.9999938011169434),\n",
       " ('adnet', 0.9999933838844299),\n",
       " ('eisec', 0.9999922513961792),\n",
       " ('us$', 0.9999921917915344)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive = ['bolsonaro'], negative = ['haddad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.77390736"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.similarity('bolsonaro', 'haddad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Doc2Vec*\n",
    "\n",
    "O *Doc2Vec* é um modelo similar ao *Word2Vec*, mas que leva em consideração também o contexto de cada frase na construção dos vetores de similaridade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import doc2vec\n",
    "\n",
    "def read_corpus(list_sentences, tokens_only=False):\n",
    "    if tokens_only:\n",
    "        return list_sentences\n",
    "    else:\n",
    "        # For training data, add tags\n",
    "        lista = []\n",
    "        for i, line in enumerate(list_sentences):\n",
    "            lista.append(doc2vec.TaggedDocument(line, [i]))\n",
    "\n",
    "        return lista\n",
    "    \n",
    "train_corpus = read_corpus(X_train_tokens)\n",
    "test_corpus = read_corpus(X_test_tokens, tokens_only=True)\n",
    "\n",
    "d2v_model = doc2vec.Doc2Vec(vector_size = 50, min_count = 2, epochs = 20)\n",
    "\n",
    "d2v_model.build_vocab(train_corpus)\n",
    "\n",
    "d2v_model.train(\n",
    "    train_corpus, \n",
    "    total_examples = d2v_model.corpus_count, \n",
    "    epochs = d2v_model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.03627553,  0.01049397,  0.02647647,  0.01406603, -0.01712412,\n",
       "       -0.05749064,  0.00441333,  0.07438195, -0.09029654, -0.02020809,\n",
       "       -0.00110922, -0.03923719, -0.0037813 ,  0.02054167, -0.02334482,\n",
       "       -0.00499761,  0.02373652,  0.0089486 , -0.04906051, -0.03406813,\n",
       "        0.01519929,  0.04696488,  0.03257749, -0.02575215,  0.04330669,\n",
       "        0.02003675, -0.01625567, -0.01424281, -0.04072778,  0.01125126,\n",
       "        0.00036247, -0.01422183, -0.00379539,  0.03486306, -0.0267536 ,\n",
       "        0.03035104, -0.00334509, -0.00667038,  0.02646112, -0.04137891,\n",
       "        0.05067763,  0.00484286, -0.02319106, -0.00397156,  0.06552546,\n",
       "       -0.00272405,  0.01524034, -0.05292337,  0.02484859,  0.01479054],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d2v_model.infer_vector(['bolsonaro', 'haddad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# medir o desempenho\n",
    "X_train_d2v = []\n",
    "\n",
    "for phrase in X_train_tokens:\n",
    "    vecs = []\n",
    "    vecs.append(d2v_model.infer_vector(phrase))\n",
    "    \n",
    "    X_train_d2v.append(vecs)\n",
    "    \n",
    "X_train_d2v = np.array(X_train_d2v)[:, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# medir o desempenho\n",
    "X_test_d2v = []\n",
    "\n",
    "for phrase in X_test_tokens:\n",
    "    vecs = []\n",
    "    vecs.append(d2v_model.infer_vector(phrase))\n",
    "    \n",
    "    X_test_d2v.append(vecs)\n",
    "    \n",
    "X_test_d2v = np.array(X_test_d2v)[:, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3500, 50)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_d2v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1500, 50)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_d2v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ecb817f0115fcb91f0cf82d46898b9ac90f725ea2a7dc8e9ce55807017ad6340"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('dsd': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
