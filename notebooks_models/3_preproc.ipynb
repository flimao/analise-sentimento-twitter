{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Pré-processamento e transformações\n",
    "\n",
    "Nessa fase, construiremos alguns modelos específicos para texto para então treiná-los;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# caminho para instalação do pacote mltoolkit, com metricas e gráficos personalizados\n",
    "# !pip install git+ssh://git@github.com/flimao/mltoolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import rcParams, rcParamsDefault, pyplot as plt\n",
    "import seaborn as sns\n",
    "from mltoolkit import metrics, plots, NLP\n",
    "import spacy\n",
    "\n",
    "rcParams.update(rcParamsDefault)\n",
    "rcParams['figure.dpi'] = 120\n",
    "rcParams['figure.figsize'] = (10, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download pt_core_news_lg\n",
    "# !python -m spacy download pt_core_news_md\n",
    "# !python -m spacy download pt_core_news_sm\n",
    "nlp = spacy.load(\"pt_core_news_lg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importação dos dados\n",
    "\n",
    "Primeiramente, importamos os dados e aplicamos as transformações utilizadas na fase anterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# não tocaremos no conjunto de submissão\n",
    "\n",
    "tweets_raw = pd.read_csv(\n",
    "    r'../data/Train3Classes.csv',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trocar tipos para acelerar o processamento (menos espaço em memória)\n",
    "# e ativar possíveis otimizações internas ao pandas para certos tipos\n",
    "def mudar_tipos(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    df['id'] = df['id'].astype('string')\n",
    "    df['tweet_date'] = pd.to_datetime(df['tweet_date'])\n",
    "    df['sentiment'] = df['sentiment'].astype('category')\n",
    "\n",
    "    return df\n",
    "\n",
    "def remover_duplicatas(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    df = df.drop_duplicates(subset = 'id')\n",
    "\n",
    "    return df\n",
    "\n",
    "# o índice é o id, visto que não há repetidos\n",
    "# vantagem: o índice é removido automaticamente quando separamos em base de treino e teste.\n",
    "def setar_index(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    df = df.set_index('id')\n",
    "\n",
    "    return df\n",
    "\n",
    "tweets_full = (tweets_raw\n",
    "    .pipe(mudar_tipos)\n",
    "    .pipe(remover_duplicatas)\n",
    "    .pipe(setar_index)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pré-processamento de texto\n",
    "\n",
    "Vamos então implementar o pré-processamento do texto da fase anterior (Análise Exploratória de Texto).\n",
    "\n",
    "Primeiramente vamos importar as *stopwords*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'../data/stopwords_alopes.txt', encoding = 'utf8') as stopword_list:\n",
    "    lst = stopword_list.read().splitlines()\n",
    "\n",
    "stopwords_alopes = set([ stopword.strip() for stopword in lst ])\n",
    "\n",
    "# em uma análise de sentimento, não queremos remover palavras com conotação negativa\n",
    "remover_stopwords = {\n",
    "    'não', \n",
    "}\n",
    "\n",
    "stopwords_alopes -= remover_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_full = lambda s: NLP.preprocessing(s, preproc_funs_args = [\n",
    "    NLP.remove_links,\n",
    "    NLP.remove_hashtags,\n",
    "    NLP.remove_mentions,\n",
    "    NLP.remove_numbers,\n",
    "    NLP.remove_special_caract,\n",
    "    NLP.lowercase,\n",
    "    #remove_punkt,\n",
    "    #(remove_stopwords, dict(stopword_list = stopword_list_alopes)),\n",
    "    (NLP.tokenize_remove_stopwords_get_radicals_spacy, dict(\n",
    "        nlp = nlp,\n",
    "        stopword_list = stopwords_alopes,\n",
    "    )),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos então aplicar esse pré-processamento a uma amostra da base de *tweets* (para podermos iterar rapidamente caso necessário). \n",
    "\n",
    "Em um momento posterior, treinaremos a base completa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "amostra_eda = 5000\n",
    "radicais = tweets_full.sample(amostra_eda)['tweet_text'].apply(preprocessing_full)\n",
    "\n",
    "tweets = tweets_full.copy()\n",
    "tweets['radicais'] = radicais\n",
    "tweets = tweets[tweets.radicais.notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>tweet_date</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>query_used</th>\n",
       "      <th>radicais</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1045267908656541697</th>\n",
       "      <td>Pai é morto na frente da família após tentar p...</td>\n",
       "      <td>2018-09-27 11:04:24+00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>g1</td>\n",
       "      <td>pai morto frente familia apo proteger filhar m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1046961086115844096</th>\n",
       "      <td>Nao tenho nite e quero fechar help me :(</td>\n",
       "      <td>2018-10-02 03:12:29+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>:(</td>\n",
       "      <td>nao nite fechar help</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1046227089647718401</th>\n",
       "      <td>com muito sono porém enrolando p dormir por sa...</td>\n",
       "      <td>2018-09-30 02:35:50+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>:(</td>\n",
       "      <td>sono pôr enrolar p dormir amanhar ir muito dor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1046779663094349826</th>\n",
       "      <td>@kyokuga same tbh isso foi uma das cenas que m...</td>\n",
       "      <td>2018-10-01 15:11:34+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>:(</td>\n",
       "      <td>same tbh cena dar throw off tbh tambem ir almo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1049124205709742082</th>\n",
       "      <td>vamos esclavas!!! :D #슈주_OneMoreTime_오늘오후6시 @S...</td>\n",
       "      <td>2018-10-08 02:27:57+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>:)</td>\n",
       "      <td>ir esclavo d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1046761908517445632</th>\n",
       "      <td>@rengafffffff Sim, é bem isso... :(</td>\n",
       "      <td>2018-10-01 14:01:01+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>:(</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1037062933690425344</th>\n",
       "      <td>Youtuber que mostrou luta contra doença morre ...</td>\n",
       "      <td>2018-09-04 19:40:45+00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>exame</td>\n",
       "      <td>youtuber mostrar lutar doenca morrer ano</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1049316346629185537</th>\n",
       "      <td>@SchuldinerSieg Puta merda! :( Pra que servem ...</td>\n",
       "      <td>2018-10-08 15:11:27+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>:(</td>\n",
       "      <td>puta merda pra servir desgracados atrasar bras...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1047545781157318656</th>\n",
       "      <td>peguei a armação nova de óculos e me achei hor...</td>\n",
       "      <td>2018-10-03 17:55:51+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>:)</td>\n",
       "      <td>pegar armacao oculos achar horroroso</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1045392360627212290</th>\n",
       "      <td>@DeFerrazdede @srta_quitete @xquadrado @BrunoT...</td>\n",
       "      <td>2018-09-27 19:18:55+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>:)</td>\n",
       "      <td>mostrar garro kkkkkkkkkkkk ofender d</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                            tweet_text  \\\n",
       "id                                                                       \n",
       "1045267908656541697  Pai é morto na frente da família após tentar p...   \n",
       "1046961086115844096           Nao tenho nite e quero fechar help me :(   \n",
       "1046227089647718401  com muito sono porém enrolando p dormir por sa...   \n",
       "1046779663094349826  @kyokuga same tbh isso foi uma das cenas que m...   \n",
       "1049124205709742082  vamos esclavas!!! :D #슈주_OneMoreTime_오늘오후6시 @S...   \n",
       "1046761908517445632                @rengafffffff Sim, é bem isso... :(   \n",
       "1037062933690425344  Youtuber que mostrou luta contra doença morre ...   \n",
       "1049316346629185537  @SchuldinerSieg Puta merda! :( Pra que servem ...   \n",
       "1047545781157318656  peguei a armação nova de óculos e me achei hor...   \n",
       "1045392360627212290  @DeFerrazdede @srta_quitete @xquadrado @BrunoT...   \n",
       "\n",
       "                                   tweet_date sentiment query_used  \\\n",
       "id                                                                   \n",
       "1045267908656541697 2018-09-27 11:04:24+00:00         2         g1   \n",
       "1046961086115844096 2018-10-02 03:12:29+00:00         0         :(   \n",
       "1046227089647718401 2018-09-30 02:35:50+00:00         0         :(   \n",
       "1046779663094349826 2018-10-01 15:11:34+00:00         0         :(   \n",
       "1049124205709742082 2018-10-08 02:27:57+00:00         1         :)   \n",
       "1046761908517445632 2018-10-01 14:01:01+00:00         0         :(   \n",
       "1037062933690425344 2018-09-04 19:40:45+00:00         2      exame   \n",
       "1049316346629185537 2018-10-08 15:11:27+00:00         0         :(   \n",
       "1047545781157318656 2018-10-03 17:55:51+00:00         1         :)   \n",
       "1045392360627212290 2018-09-27 19:18:55+00:00         1         :)   \n",
       "\n",
       "                                                              radicais  \n",
       "id                                                                      \n",
       "1045267908656541697  pai morto frente familia apo proteger filhar m...  \n",
       "1046961086115844096                               nao nite fechar help  \n",
       "1046227089647718401  sono pôr enrolar p dormir amanhar ir muito dor...  \n",
       "1046779663094349826  same tbh cena dar throw off tbh tambem ir almo...  \n",
       "1049124205709742082                                       ir esclavo d  \n",
       "1046761908517445632                                                     \n",
       "1037062933690425344           youtuber mostrar lutar doenca morrer ano  \n",
       "1049316346629185537  puta merda pra servir desgracados atrasar bras...  \n",
       "1047545781157318656               pegar armacao oculos achar horroroso  \n",
       "1045392360627212290               mostrar garro kkkkkkkkkkkk ofender d  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opções de modelos\n",
    "\n",
    "Vamos agora olhar para alguns modelos que podemos utilizar.\n",
    "\n",
    "Definiremos os modelos desejados, e então procederemos à comparação dos mesmos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tweets['radicais']\n",
    "y = tweets['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size = 0.3,\n",
    "    stratify = y,\n",
    ")\n",
    "\n",
    "X_trains = {}\n",
    "X_tests = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. *Bag of Words* / `CountVectorizer`\n",
    "\n",
    "*Bag of Words* é o processo onde traduzimos o texto já tratado para uma representação numérica que faça sentido para o modelo de *Machine Learning* consiga interpretá-lo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer()\n",
    "X_trains['bow'] = cv.fit_transform(X_train).toarray()\n",
    "X_tests['bow'] = cv.transform(X_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_trains['bow']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3500, 6318)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_trains['bow'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. TF-IDF\n",
    "\n",
    "***Term Frequency and Inverse Document Frequency*** é uma tranformação onde avaliamos a relevância das palavras pela **Frequência dos Termos** e multiplicamos pelo **Inverso da Frequência nos Documentos**.\n",
    "\n",
    "Nesse contexto, um **documento** é cada um dos textos dentro de um *dataset*. Vamos entender cada um dos termos:\n",
    "\n",
    "> **TF - Term Frequency**: é a frequência de vezes que um termo/palavra aparece em cada um dos documentos analisados (isso nos ajuda a avaliar a relevância daquela palavra);\n",
    "\n",
    "> **IDF - Inverse Document Frequency**: aqui avaliamos em quantos documentos o termo/palavra aparece (dessa forma conseguimos entender a sua influência em identificar os textos);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(use_idf = True)\n",
    "\n",
    "X_trains['tfidf'] = tfidf.fit_transform(X_train).todense()\n",
    "X_tests['tfidf']  = tfidf.transform(X_test).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_trains['tfidf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3500, 6318)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_trains['tfidf'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Word2vec*\n",
    "\n",
    "O *Word2vec* ([Wikipedia](https://en.wikipedia.org/wiki/Word2vec), [Gensim](https://radimrehurek.com/gensim/models/word2vec.html)) é uma rede neural onde associa-se vetores a cada palavra. Os vetores são tais que pretendem capturar as relações semânticas entre as mesmas.\n",
    "\n",
    "Por exemplo, se tivermos em nosso vocabulário as palavras *rei*, *rainha*, *homem* e *mulher*, poderíamos fazer a seguinte operação vetorial:\n",
    "\n",
    "$ \\vec{v}_{rei} - \\vec{v}_{homem} + \\vec{v}_{mulher} = \\vec{v}_{rainha}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "X_train_tokens = X_train.str.split(' ').to_list()\n",
    "X_test_tokens = X_test.str.split(' ').to_list()\n",
    "\n",
    "w2v_model = Word2Vec(\n",
    "    sentences = X_train_tokens, \n",
    "    vector_size = 2,  # este parâmetro é o equivalente ao número de features. \n",
    "    min_count = 1, \n",
    "    workers = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('apartamento', 1.0000001192092896),\n",
       " ('casal', 0.9999997615814209),\n",
       " ('enriquecimento', 0.9999997019767761),\n",
       " ('who', 0.9999993443489075),\n",
       " ('witzel', 0.9999993443489075),\n",
       " ('usp-sao', 0.9999991059303284),\n",
       " ('atua', 0.9999940991401672),\n",
       " ('tuite', 0.9999898076057434),\n",
       " ('risada', 0.9999882578849792),\n",
       " ('terreo', 0.9999875426292419)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive = ['bolsonaro'], negative = ['haddad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9563616"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.similarity('bolsonaro', 'haddad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# função para, dado um modelo word2vec e um conjunto de frases em formato de token (listas de listas), \n",
    "# construir os vetores associados a cada uma\n",
    "\n",
    "def build_word2vec_vectors(model, phrases, vector_combination):\n",
    "\n",
    "    X = []\n",
    "    vector_size = model.vector_size\n",
    "\n",
    "    for phrase in phrases:\n",
    "\n",
    "        ntokens = len(phrase)\n",
    "        vectors = np.zeros(shape = (ntokens, vector_size))\n",
    "\n",
    "        for i, token in enumerate(phrase):\n",
    "            try:\n",
    "\n",
    "                vectors[i, :] = model.wv[token]\n",
    "            except KeyError:  # token not present in corpus\n",
    "                vectors[i, :] = 0\n",
    "\n",
    "        X.append(vector_combination(vectors))\n",
    "    \n",
    "    return np.asarray(X)\n",
    "\n",
    "# função para, dados conjuntos de frases de treino e teste, construir os vetores\n",
    "# associados\n",
    "def build_word2vec_model(\n",
    "    X_train, X_test, \n",
    "    vector_combination,\n",
    "    is_token = False,\n",
    "    **kwargs\n",
    "):\n",
    "    # kwargs = arguments for Word2Vec class\n",
    "    \n",
    "    if is_token:\n",
    "        X_train_tokens = X_train\n",
    "        X_test_tokens = X_test\n",
    "    else:\n",
    "        X_train_tokens = X_train.str.split(' ').to_list()\n",
    "        X_test_tokens = X_test.str.split(' ').to_list()\n",
    "    \n",
    "    # instantiate, build and train model\n",
    "    w2v_model = Word2Vec(\n",
    "        sentences = X_train_tokens, \n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    # build vectors\n",
    "    X_train_w2v = build_word2vec_vectors(\n",
    "        model = w2v_model, \n",
    "        phrases = X_train_tokens, \n",
    "        vector_combination = vector_combination\n",
    "    )\n",
    "\n",
    "    X_test_w2v = build_word2vec_vectors(\n",
    "        model = w2v_model, \n",
    "        phrases = X_test_tokens, \n",
    "        vector_combination = vector_combination\n",
    "    )\n",
    "\n",
    "    return w2v_model, X_train_w2v, X_test_w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model_sum, X_trains['word2vec_sum'], X_tests['word2vec_sum'] = build_word2vec_model(\n",
    "    X_train, X_test, \n",
    "    is_token = False,\n",
    "    # --- word2vec model parameters\n",
    "    vector_size = 50, # este parâmetro é o equivalente ao número de features. \n",
    "    min_count = 2, workers = 2,\n",
    "    # --- vector_combination\n",
    "    vector_combination = lambda x: np.sum(x, axis = 0),\n",
    ")\n",
    "\n",
    "w2v_model_mean, X_trains['word2vec_mean'], X_tests['word2vec_mean'] = build_word2vec_model(\n",
    "    X_train, X_test, \n",
    "    is_token = False,\n",
    "    # --- word2vec model parameters\n",
    "    vector_size = 50, # este parâmetro é o equivalente ao número de features. \n",
    "    min_count = 2, workers = 2,\n",
    "    # --- vector_combination\n",
    "    vector_combination = lambda x: np.mean(x, axis = 0),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.06305836, -0.20200952, -0.09266718,  0.20211539, -0.3365239 ,\n",
       "        -0.67118025,  0.91174768,  1.29723158, -1.2561081 , -0.26237645,\n",
       "        -0.46331651, -0.8443768 ,  0.09114449,  0.2859693 , -0.59503645,\n",
       "         0.19414936,  0.60102654,  0.25231675, -1.09676856, -0.74585192,\n",
       "         0.30364978,  0.63270831,  0.96508832, -0.3080457 ,  0.44224393,\n",
       "         0.26311895, -0.75946613, -0.05639867, -0.9584822 ,  0.0464241 ,\n",
       "         0.33235208,  0.12054321, -0.07041632,  0.20535993, -0.48483081,\n",
       "         0.59237931,  0.55151641,  0.14408598,  0.21051726, -0.43548177,\n",
       "         0.76515405, -0.38607587, -0.20458422, -0.07331788,  1.31086324,\n",
       "         0.2223587 , -0.17039777, -0.68101051,  0.69712637,  0.25459542],\n",
       "       [-0.00804764, -0.05777715, -0.08113232,  0.08375977, -0.1659744 ,\n",
       "        -0.27797719,  0.39942197,  0.59975064, -0.51845135, -0.13187836,\n",
       "        -0.2138146 , -0.34640594,  0.03107198,  0.12967061, -0.34072803,\n",
       "         0.13442201,  0.24537586,  0.13270103, -0.46980482, -0.3099684 ,\n",
       "         0.16920361,  0.27344726,  0.4430417 , -0.1017872 ,  0.19747064,\n",
       "         0.09148164, -0.30224257, -0.0811657 , -0.44241466,  0.08232718,\n",
       "         0.13129257,  0.06339366, -0.00902611,  0.08709803, -0.23974046,\n",
       "         0.27169335,  0.24384198,  0.07071514,  0.10114265, -0.22462736,\n",
       "         0.32249739, -0.14413491, -0.04875028, -0.08117031,  0.58001001,\n",
       "         0.08574947, -0.05573931, -0.31546536,  0.31351593,  0.10354788]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_trains['word2vec_sum'][:2, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Doc2Vec*\n",
    "\n",
    "O *Doc2Vec* ([Gensim](https://radimrehurek.com/gensim/auto_examples/tutorials/run_doc2vec_lee.html)) é um modelo similar ao *Word2Vec*, mas que leva em consideração também o contexto de cada frase na construção dos vetores de similaridade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import doc2vec\n",
    "\n",
    "# função para ler o corpus e tagear os documentos (no caso, tweets)\n",
    "def read_corpus(list_sentences, tokens_only = False):\n",
    "    if tokens_only:\n",
    "        return list_sentences\n",
    "    else:\n",
    "        # For training data, add tags\n",
    "        lista = []\n",
    "        for i, line in enumerate(list_sentences):\n",
    "            lista.append(doc2vec.TaggedDocument(line, [i]))\n",
    "\n",
    "        return lista\n",
    "    \n",
    "train_corpus = read_corpus(X_train_tokens)\n",
    "test_corpus = read_corpus(X_test_tokens, tokens_only = True)\n",
    "\n",
    "d2v_model = doc2vec.Doc2Vec(vector_size = 50, min_count = 2, epochs = 20)\n",
    "\n",
    "d2v_model.build_vocab(train_corpus)\n",
    "\n",
    "d2v_model.train(\n",
    "    train_corpus, \n",
    "    total_examples = d2v_model.corpus_count, \n",
    "    epochs = d2v_model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01400302, -0.02584669,  0.00031836, -0.00150514, -0.01399519,\n",
       "       -0.02254681,  0.03149108,  0.03564623, -0.06478699, -0.03257909,\n",
       "       -0.03688202, -0.02355443,  0.00209444, -0.00101186, -0.03051149,\n",
       "        0.02547154,  0.01096833, -0.00045831, -0.03104848, -0.02308318,\n",
       "        0.00204974,  0.03852638,  0.04322977, -0.02161267,  0.03768882,\n",
       "        0.02182475, -0.04520045,  0.00242103, -0.04459028,  0.01308559,\n",
       "        0.01355772,  0.01562951, -0.00284459,  0.04227247, -0.02978106,\n",
       "        0.04166187,  0.0138301 ,  0.00326481,  0.02070507, -0.02589211,\n",
       "        0.02491639,  0.01667134,  0.00890227, -0.00474655,  0.03985274,\n",
       "        0.0103071 , -0.01104184, -0.0334091 ,  0.03692046, -0.00392407],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exemplo: vetor de uma frase contendo duas palavras: 'bolsonaro' e 'haddad'\n",
    "\n",
    "d2v_model.infer_vector(['bolsonaro', 'haddad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# função para, dado um modelo doc2vec e um conjunto de tokens, construir o vetor associado\n",
    "def build_doc2vec_vector(d2v_model, phrases):\n",
    "    X = []\n",
    "\n",
    "    for phrase in phrases:\n",
    "        vecs = []\n",
    "        vecs.append(d2v_model.infer_vector(phrase))\n",
    "        \n",
    "        X.append(vecs)\n",
    "        \n",
    "    X_d2v = np.array(X)[:, 0, :]\n",
    "\n",
    "    return X_d2v\n",
    "\n",
    "# função para, dados conjuntos de frases de treino e teste, construir os vetores\n",
    "# associados\n",
    "\n",
    "def build_doc2vec_model(\n",
    "    X_train, X_test, \n",
    "    is_token = False,\n",
    "    **kwargs\n",
    "):\n",
    "\n",
    "    if is_token:\n",
    "        X_train_tokens = X_train\n",
    "        X_test_tokens = X_test\n",
    "    else:\n",
    "        X_train_tokens = X_train.str.split(' ').to_list()\n",
    "        X_test_tokens = X_test.str.split(' ').to_list()\n",
    "    \n",
    "    # make corpus\n",
    "    train_corpus = read_corpus(X_train_tokens)\n",
    "    test_corpus = read_corpus(X_test_tokens, tokens_only = True)\n",
    "\n",
    "    # instantiate doc2vec model\n",
    "    d2v_model = doc2vec.Doc2Vec(**kwargs)\n",
    "\n",
    "    # build vocabulary\n",
    "    d2v_model.build_vocab(train_corpus)\n",
    "\n",
    "    # train model\n",
    "    d2v_model.train(\n",
    "        train_corpus, \n",
    "        total_examples = d2v_model.corpus_count, \n",
    "        epochs = d2v_model.epochs\n",
    "    )\n",
    "\n",
    "    # build vectors\n",
    "    X_train_d2v = build_doc2vec_vector(\n",
    "        d2v_model = d2v_model, \n",
    "        phrases = X_train_tokens\n",
    "    )\n",
    "    X_test_d2v = build_doc2vec_vector(\n",
    "        d2v_model = d2v_model, \n",
    "        phrases = X_test_tokens\n",
    "    )\n",
    "\n",
    "    return d2v_model, X_train_d2v, X_test_d2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2v_model, X_trains['doc2vec'], X_tests['doc2vec'] = build_doc2vec_model(\n",
    "    X_train, X_test, \n",
    "    is_token = False,\n",
    "    vector_size = 50, min_count = 2, epochs = 20,  # doc2vec model arguments\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.01307306, -0.00381729,  0.01335488,  0.01585352,  0.00773212,\n",
       "         0.00179032, -0.00681294,  0.00483858, -0.01388805,  0.0173075 ,\n",
       "         0.0064855 ,  0.01094348,  0.0075306 ,  0.00943774,  0.0010797 ,\n",
       "         0.00032177,  0.00946476,  0.00330576, -0.00337737, -0.00676753,\n",
       "        -0.02491206, -0.00238715,  0.01878353,  0.00761508, -0.00095866,\n",
       "        -0.00463589,  0.01800724, -0.00082818,  0.00514946, -0.01250828,\n",
       "        -0.00029735, -0.0079055 , -0.00708129, -0.01498468, -0.0102274 ,\n",
       "         0.00218874, -0.0064918 , -0.00637636,  0.0160045 , -0.00766212,\n",
       "        -0.01217097, -0.00655804, -0.00773143, -0.01970037,  0.00555086,\n",
       "        -0.01224645,  0.00232145, -0.01259743, -0.00470705, -0.00246265],\n",
       "       [ 0.0080996 ,  0.0099153 ,  0.00129044, -0.00274199,  0.0016918 ,\n",
       "         0.01853403,  0.00152807, -0.00361797,  0.0172362 ,  0.0057939 ,\n",
       "         0.01489335,  0.00026075, -0.00326554, -0.00418042,  0.00342851,\n",
       "         0.00361359,  0.00770666, -0.00032519,  0.01425409,  0.00139571,\n",
       "        -0.00112904, -0.01273211, -0.0102391 , -0.00286719, -0.0032809 ,\n",
       "        -0.00935303,  0.00940704,  0.00558339,  0.01187229,  0.01241233,\n",
       "        -0.00063293, -0.00111438,  0.00898524,  0.00787123, -0.0045413 ,\n",
       "        -0.01075849, -0.01430205,  0.00918329,  0.00510834, -0.00752536,\n",
       "        -0.01107871,  0.00959952,  0.00099074, -0.00696877, -0.00635889,\n",
       "        -0.01251734, -0.001475  ,  0.0003553 , -0.00901797, -0.00128858]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_trains['doc2vec'][:2, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Em resumo\n",
    "\n",
    "Modelos de aprendizado de máquina não conseguem trabalhar com texto, somente números. Temos que ter algumas técnicas para transformar o texto em números.\n",
    "\n",
    "Treinamos quatro modelos que transformam texto puro em vetores de *features* com os quais os modelos de aprendizado de máquina conseguem trabalhar:\n",
    "\n",
    "* *Bag of Words* puro, ou `CountVectorizer`;\n",
    "* *Bag of Words* TF-IDF (ou seja, considerando a influência de cada *tweet*);\n",
    "* *Word2vec* (com duas maneiras de combinar os vetores de cada palavra); e\n",
    "* *Doc2vec*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['bow', 'tfidf', 'word2vec_sum', 'word2vec_mean', 'doc2vec'])\n"
     ]
    }
   ],
   "source": [
    "print(X_trains.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O próximo passo é treinar os modelos de aprendizado de máquina para prever, dado um *tweet* novo, qual o tom (sentimento) dele: positivo, negativo ou neutro."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ecb817f0115fcb91f0cf82d46898b9ac90f725ea2a7dc8e9ce55807017ad6340"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('dsd': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
